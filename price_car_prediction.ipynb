{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study - Regression\n",
    "\n",
    "This data is a **regression problem**, trying to predict life expectancy.\n",
    "\n",
    "The followings describe the features.\n",
    "\n",
    "- **Country**\n",
    "- **Year**\n",
    "- **Status**: Developed or Developing status\n",
    "- **Life expectancy**: Life Expectancy in age\n",
    "- **Adult Mortality**: Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)\n",
    "- **Infant deaths**: Number of Infant Deaths per 1000 population\n",
    "- **Alcohol**: Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)\n",
    "- **Percentage expenditure**: Expenditure on health as a percentage of Gross Domestic Product per capita(%)\n",
    "- **Hepatitis B**: Hepatitis B (HepB) immunization coverage among 1-year-olds (%)\n",
    "- **Measles**: - number of reported cases per 1000 population\n",
    "- **BMI** Average Body Mass Index of entire population\n",
    "- **under-five deaths**: Number of under-five deaths per 1000 population\n",
    "- **Polio**: Polio (Pol3) immunization coverage among 1-year-olds (%)\n",
    "- **Total expenditure**: General government expenditure on health as a percentage of total government expenditure (%)\n",
    "- **Diphtheria**: Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)\n",
    "- **HIV/AIDS**: Deaths per 1000 live births HIV/AIDS (0-4 years)\n",
    "- **GDP**: Gross Domestic Product per capita (in USD)\n",
    "- **Population**: Population of the country\n",
    "- **thinness 1-19 years**: Prevalence of thinness among children and adolescents for Age 10 to 19 (% )\n",
    "- **thinness 5-9 years**: Prevalence of thinness among children for Age 5 to 9(%)\n",
    "- **Income composition of resources**: Human Development Index in terms of income composition of resources (index ranging from 0 to 1)\n",
    "- **Schooling**: Number of years of Schooling(years)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "np.__version__, pd.__version__, sns.__version__, matplotlib.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/car_price_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of your data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical info Hint: look up .describe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Dtypes of your input data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the column names\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e747b26a",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "EDA is an essential step to inspect the data, so to better understand nature of the given data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6ff88d3",
   "metadata": {},
   "source": [
    "### Renaming\n",
    "\n",
    "Now we would like to rename some of the following column names, so it's easy to write the code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df.rename(columns = {'name':'brand' \n",
    "                     }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5d419fc",
   "metadata": {},
   "source": [
    "### 2.1 Univariate analyis\n",
    "\n",
    "Single variable exploratory data anlaysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf616b09",
   "metadata": {},
   "source": [
    "#### Countplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd38935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many developing and developed countries there are\n",
    "sns.countplot(data = df, x = 'seats')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "239be0e2",
   "metadata": {},
   "source": [
    "#### Distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data = df, x = 'seats')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1354e62",
   "metadata": {},
   "source": [
    "### 2.2 Multivariate analysis\n",
    "\n",
    "Multiple variable exploratory data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bdd851e",
   "metadata": {},
   "source": [
    "#### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try bar plot on \"Status\"\n",
    "sns.boxplot(x = df[\"transmission\"], y = df[\"selling_price\"])\n",
    "plt.ylabel(\"selling_price\")\n",
    "plt.xlabel(\"transmission\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67889479",
   "metadata": {},
   "source": [
    "#### Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d31634",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = df['selling_price'], y = df['year'], hue=df['transmission'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a14d97ae",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "\n",
    "Let's use correlation matrix to find strong factors predicting the life expectancy.  It's also for checking whether certain features are too correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30106062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out heatmap\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")  #don't forget these are not all variables! categorical is not here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d07a2f",
   "metadata": {},
   "source": [
    "#### Tips: Label encoding\n",
    "\n",
    "Now we would like to change \"Developing\" and \"Developed\" to \"0\" and \"1\", since machine learning algorithms do not understand text.   Also, correlation matrix and other similar computational tools require label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_mapping = {\n",
    "    'First Owner': 1,\n",
    "    'Second Owner': 2,\n",
    "    'Third Owner': 3,\n",
    "    'Fourth & Above Owner': 4,\n",
    "    'Test Drive Car': 5\n",
    "}\n",
    "\n",
    "categorical_data = df['owner']\n",
    "\n",
    "df['owner'] = [label_mapping[label] for label in categorical_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56fdcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['fuel'].isin(['CNG', 'LPG'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric mileage values by splitting and converting to float\n",
    "df['mileage'] = df['mileage'].str.split().str[0].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfa1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"CC\" and convert to float\n",
    "df['engine'] = df['engine'].str.replace(' CC', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \" bhp\" and convert to float, handling N/A values\n",
    "df['max_power'] = df['max_power'].str.replace(' bhp', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff662ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first word and update the column\n",
    "df['brand'] = df['brand'].apply(lambda x: x.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'Diesel': 1,\n",
    "    'Petrol': 2\n",
    "}\n",
    "\n",
    "categorical_data1 = df['fuel']\n",
    "\n",
    "df['lable_fuel'] = [label_mapping[label] for label in categorical_data1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'torque' feature\n",
    "df = df.drop(columns=['torque'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with 'Test Drive Cars' in the 'make' column\n",
    "df = df[df['owner'] != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37745d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['log_selling_price'] = np.log(df['selling_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out heatmap\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")  #don't forget these are not all variables! categorical is not here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09b22139",
   "metadata": {},
   "source": [
    "#### Predictive Power Score\n",
    "\n",
    "This is another way to check the predictive power of some feature.  Unlike correlation, `pps` actually obtained from actual prediction.  For more details:\n",
    "    \n",
    "- The score is calculated using only 1 feature trying to predict the target column. This means there are no interaction effects between the scores of various features. Note that this is in contrast to feature importance\n",
    "- The score is calculated on the test sets of a 4-fold crossvalidation (number is adjustable via `ppscore.CV_ITERATIONS`)\n",
    "- All rows which have a missing value in the feature or the target column are dropped\n",
    "- In case that the dataset has more than 5,000 rows the score is only calculated on a random subset of 5,000 rows with a fixed random seed (`ppscore.RANDOM_SEED`). You can adjust the number of rows or skip this sampling via the API. However, in most scenarios the results will be very similar.\n",
    "- There is no grid search for optimal model parameters\n",
    "\n",
    "We can install by doing <code>pip install ppscore</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppscore as pps\n",
    "\n",
    "# before using pps, let's drop country and year\n",
    "dfcopy = df.copy()\n",
    "dfcopy.drop(['year'], axis='columns', inplace=True)\n",
    "\n",
    "#this needs some minor preprocessing because seaborn.heatmap unfortunately does not accept tidy data\n",
    "matrix_df = pps.matrix(dfcopy)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbf3abf4",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We gonna skip for this tutorial.  But we can certainly try to combine some columsn to create new features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0160f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x is our strong features\n",
    "X = df[['max_power','engine','mileage','seats', 'km_driven', 'owner', 'lable_fuel']]\n",
    "\n",
    "#y is simply the life expectancy col\n",
    "y = df[\"log_selling_price\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9401f85",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56eb18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42) #It can be 0.2-0.4 for test_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acfea971",
   "metadata": {},
   "source": [
    "## 5. Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbd4215d",
   "metadata": {},
   "source": [
    "### Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "X_train[['max_power']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c17fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['max_power']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "X_train[['engine']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['engine']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc081e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "X_train[['mileage']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['mileage']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12abb84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "X_train[['seats']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['seats']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae739169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "X_train[['km_driven']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['km_driven']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c94798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "X_train[['owner']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6df80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['owner']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "X_train[['lable_fuel']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['lable_fuel']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c22c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb06f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c19471",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='max_power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83783457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_power'].mean(),df['max_power'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f861942",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['engine'].mean(),df['engine'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ed467",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='mileage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mileage'].mean(),df['mileage'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31179d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='seats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seats'].mean(),df['seats'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='km_driven')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81495f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['km_driven'].mean(),df['km_driven'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='owner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f76ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['owner'].mean(),df['owner'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='lable_fuel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10572379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lable_fuel'].mean(),df['lable_fuel'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f67edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_selling_price'].mean(),df['log_selling_price'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0ce57",
   "metadata": {},
   "source": [
    "Mean: Use the mean to fill missing values if the data is approximately normally distributed and does not have significant outliers. The mean is sensitive to extreme values, so if your data has outliers, using the mean might result in skewed imputations.\n",
    "\n",
    "Median: Use the median to fill missing values if your data has outliers or is skewed. The median is a robust measure of central tendency and is less affected by extreme values compared to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fill the training set first!\n",
    "\n",
    "X_train['max_power'].fillna(X_train['max_power'].median(), inplace=True)\n",
    "X_train['engine'].fillna(X_train['engine'].median(), inplace=True)\n",
    "X_train['mileage'].fillna(X_train['mileage'].median(), inplace=True)\n",
    "X_train['seats'].fillna(X_train['seats'].median(), inplace=True)\n",
    "X_train['km_driven'].fillna(X_train['km_driven'].median(), inplace=True)\n",
    "X_train['owner'].fillna(X_train['owner'].mean(), inplace=True)\n",
    "X_train['lable_fuel'].fillna(X_train['lable_fuel'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fill the testing set with the training distribution first!\n",
    "\n",
    "X_test['max_power'].fillna(X_train['max_power'].median(), inplace=True)\n",
    "X_test['engine'].fillna(X_train['engine'].median(), inplace=True)\n",
    "X_test['mileage'].fillna(X_train['mileage'].median(), inplace=True)\n",
    "X_test['seats'].fillna(X_train['seats'].median(), inplace=True)\n",
    "X_test['km_driven'].fillna(X_train['km_driven'].median(), inplace=True)\n",
    "X_test['owner'].fillna(X_train['owner'].mean(), inplace=True)\n",
    "X_test['lable_fuel'].fillna(X_train['lable_fuel'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4926526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for y\n",
    "y_train.fillna(y_train.median(), inplace=True)\n",
    "y_test.fillna(y_train.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check again\n",
    "X_train[['max_power']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc75e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['max_power']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['engine']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['engine']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['mileage']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['mileage']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['seats']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['seats']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['km_driven']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab62e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['km_driven']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b464de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['owner']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['owner']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9bd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['lable_fuel']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ae448",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['lable_fuel']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum(), y_test.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b80cd03",
   "metadata": {},
   "source": [
    "### Checking Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb13273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of columns.\n",
    "col_dict = {'max_power':1,'engine':2,'mileage':3,'seats':4, 'km_driven':5, 'owner':6, 'lable_fuel':7}\n",
    "\n",
    "# Detect outliers in each variable using box plots.\n",
    "plt.figure(figsize=(20,30))\n",
    "\n",
    "for variable,i in col_dict.items():\n",
    "                     plt.subplot(5,4,i)\n",
    "                     plt.boxplot(X_train[variable])\n",
    "                     plt.title(variable)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d646458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_count(col, data = X_train):\n",
    "    \n",
    "    # calculate your 25% quatile and 75% quatile\n",
    "    q75, q25 = np.percentile(data[col], [75, 25])\n",
    "    \n",
    "    # calculate your inter quatile\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    # min_val and max_val\n",
    "    min_val = q25 - (iqr*1.5)\n",
    "    max_val = q75 + (iqr*1.5)\n",
    "    \n",
    "    # count number of outliers, which are the data that are less than min_val or more than max_val calculated above\n",
    "    outlier_count = len(np.where((data[col] > max_val) | (data[col] < min_val))[0])\n",
    "    \n",
    "    # calculate the percentage of the outliers\n",
    "    outlier_percent = round(outlier_count/len(data[col])*100, 2)\n",
    "    \n",
    "    if(outlier_count > 0):\n",
    "        print(\"\\n\"+15*'-' + col + 15*'-'+\"\\n\")\n",
    "        print('Number of outliers: {}'.format(outlier_count))\n",
    "        print('Percent of data that is outlier: {}%'.format(outlier_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daac8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train.columns:\n",
    "    outlier_count(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3f8db3b",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39937bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# feature scaling helps improve reach convergence faster\n",
    "scaler = StandardScaler() # for standadization use StandardScaler() , for normalization use MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "#x = (x - mean) / std\n",
    "#why do we want to scale our data before data analysis / machine learning\n",
    "\n",
    "#allows your machine learning model to catch the pattern/relationship faster\n",
    "#faster convergence\n",
    "\n",
    "#how many ways to scale\n",
    "#standardardization <====current way\n",
    "# (x - mean) / std\n",
    "#--> when your data follows normal distribution\n",
    "\n",
    "#normalization <---another way\n",
    "# (x - x_min) / (x_max - x_min)\n",
    "#---> when your data DOES NOT follow normal distribution (e.g., audio, signal, image) We will use nomalization when mean is a bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check shapes of all X_train, X_test, y_train, y_test\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37ec9dd2",
   "metadata": {},
   "source": [
    "## 6. Modeling\n",
    "\n",
    "Let's define some algorithms and compare them using cross-validation.\n",
    "\n",
    "[Scikit-Learn](http://scikit-learn.org) provides quick access to a huge pool of machine learning algorithms.\n",
    "\n",
    "Before using sklearn, there is **one thing you need to know**, i.e., the **data shape that sklearn wants**.\n",
    "\n",
    "To apply majority of the algorithms, sklearn requires two inputs, i.e., $\\mathbf{X}$ and $\\mathbf{y}$.\n",
    "\n",
    "-  $\\mathbf{X}$, or the **feature matrix** *typically* has the shape of ``[n_samples, n_features]``\n",
    "-  $\\mathbf{y}$, or the **target/label vector** *typically* has the shape of ``[n_samples, ]`` or ``[n_samples, n_targets]`` depending whether that algorithm supports multiple labels\n",
    "\n",
    "Note 1:  if you $\\mathbf{X}$ has only 1 feature, the shape must be ``[n_samples, 1]`` NOT ``[n_samples, ]``\n",
    "\n",
    "Note 2:  sklearn supports both numpy and pandas, as long as the shape is right.  For example, if you use pandas, $\\mathbf{X}$ would be a dataframe, and $\\mathbf{y}$ could be a series or dataframe.\n",
    "\n",
    "Tips:  it's always better to look at sklearn documentation before applying any algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0efd13a",
   "metadata": {},
   "source": [
    "### Much better: Cross validation + Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  #we are using regression models\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Libraries for model evaluation\n",
    "\n",
    "# models that we will be using, put them in a list\n",
    "algorithms = [LinearRegression(), SVR(), KNeighborsRegressor(), DecisionTreeRegressor(random_state = 0), \n",
    "              RandomForestRegressor(n_estimators = 100, random_state = 0)]\n",
    "\n",
    "# The names of the models\n",
    "algorithm_names = [\"Linear Regression\", \"SVR\", \"KNeighbors Regressor\", \"Decision-Tree Regressor\", \"Random-Forest Regressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea954396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  #we are using regression models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print(\"MSE: \", mean_squared_error(y_test, yhat))\n",
    "print(\"r2: \", r2_score(y_test, yhat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "473e2531",
   "metadata": {},
   "source": [
    "Let's do some simple cross-validation here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85366d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "#lists for keeping mse\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "#defining splits\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for i, model in enumerate(algorithms):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error') #Higher is better.\n",
    "    print(f\"{algorithm_names[i]} - Score: {scores}; Mean: {scores.mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeb53bde",
   "metadata": {},
   "source": [
    "Hmm...it seems random forest do very well....how about we grid search further to find the best version of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6db1e5e9",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a220979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'bootstrap': [True], 'max_depth': [5, 10, None],\n",
    "              'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 1)\n",
    "\n",
    "grid = GridSearchCV(estimator = rf, \n",
    "                    param_grid = param_grid, \n",
    "                    cv = kfold, \n",
    "                    n_jobs = -1, \n",
    "                    return_train_score=True, \n",
    "                    refit=True,\n",
    "                    scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit your grid_search\n",
    "grid.fit(X_train, y_train);  #fit means start looping all the possible parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae371c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find your grid_search's best score\n",
    "best_mse = grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse  # ignore the minus because it's neg_mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ff42019",
   "metadata": {},
   "source": [
    "## 7. Testing\n",
    "\n",
    "Of course, once we do everything.  We can try to shoot with the final test set.  We should no longer do anything like improving the model.  It's illegal!  since X_test is the final final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f82a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = grid.predict(X_test)\n",
    "mean_squared_error(y_test,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72562afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original = np.exp(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b70535",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_original)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6074023",
   "metadata": {},
   "source": [
    "## 8. Analysis:  Feature Importance\n",
    "\n",
    "Understanding why is **key** to every business, not how low MSE we got.  Extracting which feature is important for prediction can help us interpret the results.  There are several ways: algorithm, permutation, and shap.  Note that these techniques can be mostly applied to most algorithms. \n",
    "\n",
    "Most of the time, we just apply all, and check the consistency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "073fb9a1",
   "metadata": {},
   "source": [
    "#### Algorithm way\n",
    "\n",
    "Some ML algorithms provide feature importance score after you fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bed781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stored in this variable\n",
    "#note that grid here is random forest\n",
    "rf = grid.best_estimator_\n",
    "\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828da6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot\n",
    "plt.barh(X.columns, rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88aac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmm...let's sort first\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "plt.barh(X.columns[sorted_idx], rf.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Random-Forest Regressor Importance\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2398c6b",
   "metadata": {},
   "source": [
    "#### Permutation way\n",
    "\n",
    "This method will randomly shuffle each feature and compute the change in the model’s performance. The features which impact the performance the most are the most important one.\n",
    "\n",
    "*Note*: The permutation based importance is computationally expensive. The permutation based method can have problem with highly-correlated features, it can report them as unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(rf, X_test, y_test)\n",
    "\n",
    "#let's plot\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(X.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Random-Forest Regressor Importance\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d076c453",
   "metadata": {},
   "source": [
    "#### Shap way\n",
    "\n",
    "The SHAP interpretation can be used (it is model-agnostic) to compute the feature importances. It is using the Shapley values from game theory to estimate the how does each feature contribute to the prediction. It can be easily installed (<code>pip install shap</code>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap provides plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names = X.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e5ac18e",
   "metadata": {},
   "source": [
    "## 9. Inference\n",
    "\n",
    "To provide inference service or deploy, it's best to save the model for latter use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'model/price_car_prediction.model'\n",
    "pickle.dump(grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543edf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcfc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try to create one silly example\n",
    "df[['max_power','engine','mileage','seats', 'km_driven', 'owner', 'lable_fuel','selling_price']].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array([[103.52,1498.00,21.14,5.00,120000.00,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a49f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "selling_price = loaded_model.predict(sample)\n",
    "selling_price = np.exp(selling_price)\n",
    "print(selling_price)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
