{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1: Predicting Car Price\n",
    "\n",
    "This data is a **regression problem**, trying to predict car price.\n",
    "\n",
    "The followings describe the features.\n",
    "\n",
    " name : brandname of the car        \n",
    " year : released year car\n",
    " selling_price : price for buying the car\n",
    " km_driven : kilometer that the car has been driven      \n",
    " fuel : fuel that the car can used          \n",
    " seller_type : someone that customer buy the car     \n",
    " transmission : manual or anutomatic    \n",
    " owner : the number of owners that how many people used it.          \n",
    " mileage : a term use to express the fuel efficiency of a vehicle.       \n",
    " engine : size of engine         \n",
    " max_power : maximum power output that car can make.     \n",
    " torque : a measurement of your car's ability to do work.            \n",
    " seats : the capacity that one car can carry people."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the important libraries\n",
    "import numpy as np #Data manipulation\n",
    "import pandas as pd #open file like Exel\n",
    "import seaborn as sns #Data visualization\n",
    "import matplotlib.pyplot as plt #Data visualization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "np.__version__, pd.__version__, sns.__version__, matplotlib.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data from path that save in computer.\n",
    "df = pd.read_csv('data/car_price_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of your data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical info Hint: look up .describe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Dtypes of your input data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the column names\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e747b26a",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "EDA is an essential step to inspect the data, so to better understand nature of the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the column. What we have in the dataset.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns named 'name' to 'brand'\n",
    "df.rename(columns = {'name':'brand' \n",
    "                     }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check again\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5d419fc",
   "metadata": {},
   "source": [
    "### 2.1 Univariate analyis\n",
    "\n",
    "Single variable exploratory data anlaysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf616b09",
   "metadata": {},
   "source": [
    "#### Countplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd38935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many developing and developed countries there are\n",
    "sns.countplot(data = df, x = 'engine')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "239be0e2",
   "metadata": {},
   "source": [
    "#### Distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data = df, x = 'engine')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1354e62",
   "metadata": {},
   "source": [
    "### 2.2 Multivariate analysis\n",
    "\n",
    "Multiple variable exploratory data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bdd851e",
   "metadata": {},
   "source": [
    "#### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try bar plot on \"Status\"\n",
    "sns.boxplot(x = df[\"transmission\"], y = df[\"selling_price\"])\n",
    "plt.ylabel(\"selling_price\")\n",
    "plt.xlabel(\"transmission\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67889479",
   "metadata": {},
   "source": [
    "#### Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d31634",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = df['selling_price'], y = df['year'], hue=df['transmission'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a14d97ae",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "\n",
    "Let's use correlation matrix to find strong factors predicting the life expectancy.  It's also for checking whether certain features are too correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30106062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out heatmap\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")  #don't forget these are not all variables! categorical is not here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d07a2f",
   "metadata": {},
   "source": [
    "#### Tips: Label encoding\n",
    "\n",
    "Now we would like to change \"Developing\" and \"Developed\" to \"0\" and \"1\", since machine learning algorithms do not understand text.   Also, correlation matrix and other similar computational tools require label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the feature owner, map First owner to 1, ..., Test Drive Car to 5\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_mapping = {\n",
    "    'First Owner': 1,\n",
    "    'Second Owner': 2,\n",
    "    'Third Owner': 3,\n",
    "    'Fourth & Above Owner': 4,\n",
    "    'Test Drive Car': 5\n",
    "}\n",
    "\n",
    "categorical_data = df['owner']\n",
    "\n",
    "df['owner'] = [label_mapping[label] for label in categorical_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e23d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56fdcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the feature fuel, remove all rows with CNG and LPG because CNG and LPG use a different mileage system \n",
    "# i.e., km/kg which is different from kmfeaturepl for Diesel and Petrol\n",
    "df = df[~df['fuel'].isin(['CNG', 'LPG'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the feature mileage, remove “kmpl” and convert the column to numerical type (e.g., float).\n",
    "# Extract numeric mileage values by splitting and converting to float\n",
    "df['mileage'] = df['mileage'].str.split().str[0].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfa1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"CC\" and convert to float\n",
    "df['engine'] = df['engine'].str.replace(' CC', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \" bhp\" and convert to float, handling N/A values\n",
    "df['max_power'] = df['max_power'].str.replace(' bhp', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff662ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the brand name is followed by a space\n",
    "df['brand'] = df['brand'].str.split(' ', n=1).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert string to numeric\n",
    "label_mapping = {\n",
    "    'Diesel': 1,\n",
    "    'Petrol': 2\n",
    "}\n",
    "\n",
    "categorical_data1 = df['fuel']\n",
    "\n",
    "df['lable_fuel'] = [label_mapping[label] for label in categorical_data1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'torque' feature\n",
    "df = df.drop(columns=['torque'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with 'Test Drive Cars' in the 'make' column\n",
    "df = df[df['owner'] != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37745d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#because selling_price is very high. It may cause prediction to be very unstable.To solve this we will put log to the data.\n",
    "df['log_selling_price'] = np.log(df['selling_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Check again\n",
    "df.head(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out heatmap\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")  #don't forget these are not all variables! categorical is not here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09b22139",
   "metadata": {},
   "source": [
    "#### Predictive Power Score\n",
    "\n",
    "This is another way to check the predictive power of some feature.  Unlike correlation, `pps` actually obtained from actual prediction.  For more details:\n",
    "    \n",
    "- The score is calculated using only 1 feature trying to predict the target column. This means there are no interaction effects between the scores of various features. Note that this is in contrast to feature importance\n",
    "- The score is calculated on the test sets of a 4-fold crossvalidation (number is adjustable via `ppscore.CV_ITERATIONS`)\n",
    "- All rows which have a missing value in the feature or the target column are dropped\n",
    "- In case that the dataset has more than 5,000 rows the score is only calculated on a random subset of 5,000 rows with a fixed random seed (`ppscore.RANDOM_SEED`). You can adjust the number of rows or skip this sampling via the API. However, in most scenarios the results will be very similar.\n",
    "- There is no grid search for optimal model parameters\n",
    "\n",
    "We can install by doing <code>pip install ppscore</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppscore as pps\n",
    "\n",
    "# before using pps, let's drop country and year\n",
    "dfcopy = df.copy()\n",
    "#dfcopy.drop(['year'], axis='columns', inplace=True)\n",
    "\n",
    "#this needs some minor preprocessing because seaborn.heatmap unfortunately does not accept tidy data\n",
    "matrix_df = pps.matrix(dfcopy)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbf3abf4",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We gonna skip for this tutorial.  But we can certainly try to combine some columsn to create new features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0160f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x is our strong features\n",
    "X = df[['max_power','mileage','seats', 'km_driven', 'owner', 'lable_fuel']]\n",
    "\n",
    "#y is simply the life expectancy col\n",
    "y = df[\"log_selling_price\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9401f85",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56eb18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42) #It can be 0.2-0.4 for test_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acfea971",
   "metadata": {},
   "source": [
    "## 5. Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbd4215d",
   "metadata": {},
   "source": [
    "### Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "X_train[['max_power']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c17fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['max_power']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train[['engine']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test[['engine']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc081e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['mileage']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['mileage']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12abb84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['seats']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['seats']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae739169",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['km_driven']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['km_driven']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c94798",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['owner']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6df80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['owner']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['lable_fuel']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['lable_fuel']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c22c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb06f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66394cea",
   "metadata": {},
   "source": [
    "##### Plot the graph then calculated mean and median for imputation. (filling missing value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c19471",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='max_power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83783457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_power'].mean(),df['max_power'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f861942",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['engine'].mean(),df['engine'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ed467",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='mileage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mileage'].mean(),df['mileage'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31179d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='seats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seats'].mean(),df['seats'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='km_driven')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81495f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['km_driven'].mean(),df['km_driven'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='owner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f76ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['owner'].mean(),df['owner'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='lable_fuel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10572379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lable_fuel'].mean(),df['lable_fuel'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f67edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_selling_price'].mean(),df['log_selling_price'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0ce57",
   "metadata": {},
   "source": [
    "Mean: Use the mean to fill missing values if the data is approximately normally distributed and does not have significant outliers. The mean is sensitive to extreme values, so if your data has outliers, using the mean might result in skewed imputations.\n",
    "\n",
    "Median: Use the median to fill missing values if your data has outliers or is skewed. The median is a robust measure of central tendency and is less affected by extreme values compared to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fill missing values for the training set first!\n",
    "X_train['max_power'].fillna(X_train['max_power'].median(), inplace=True) #median is a good representation\n",
    "X_train['mileage'].fillna(X_train['mileage'].median(), inplace=True) #median is a good representation\n",
    "X_train['seats'].fillna(X_train['seats'].median(), inplace=True) #median is a good representation\n",
    "X_train['km_driven'].fillna(X_train['km_driven'].median(), inplace=True) #median is a good representation\n",
    "X_train['owner'].fillna(X_train['owner'].mean(), inplace=True) #mean is a good representation\n",
    "X_train['lable_fuel'].fillna(X_train['lable_fuel'].median(), inplace=True) #median is a good representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fill the testing set with the training distribution first!\n",
    "X_test['max_power'].fillna(X_train['max_power'].median(), inplace=True) #median is a good representation\n",
    "X_test['mileage'].fillna(X_train['mileage'].median(), inplace=True) #median is a good representation\n",
    "X_test['seats'].fillna(X_train['seats'].median(), inplace=True) #median is a good representation\n",
    "X_test['km_driven'].fillna(X_train['km_driven'].median(), inplace=True) #median is a good representation\n",
    "X_test['owner'].fillna(X_train['owner'].mean(), inplace=True) #mean is a good representation\n",
    "X_test['lable_fuel'].fillna(X_train['lable_fuel'].median(), inplace=True) #median is a good representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4926526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for y\n",
    "#y_train.fillna(y_train.median(), inplace=True)\n",
    "#y_test.fillna(y_train.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check again\n",
    "X_train[['max_power']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc75e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['max_power']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['mileage']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['mileage']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['seats']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['seats']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['km_driven']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab62e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['km_driven']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b464de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['owner']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['owner']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9bd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['lable_fuel']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ae448",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['lable_fuel']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum(), y_test.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b80cd03",
   "metadata": {},
   "source": [
    "### Checking Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb13273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of columns.\n",
    "col_dict = {'max_power':1,'mileage':2,'seats':3, 'km_driven':4, 'owner':5, 'lable_fuel':6}\n",
    "\n",
    "# Detect outliers in each variable using box plots.\n",
    "plt.figure(figsize=(20,30))\n",
    "\n",
    "for variable,i in col_dict.items():\n",
    "                     plt.subplot(5,4,i)\n",
    "                     plt.boxplot(X_train[variable])\n",
    "                     plt.title(variable)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d646458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_count(col, data = X_train):\n",
    "    \n",
    "    # calculate your 25% quatile and 75% quatile\n",
    "    q75, q25 = np.percentile(data[col], [75, 25])\n",
    "    \n",
    "    # calculate your inter quatile\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    # min_val and max_val\n",
    "    min_val = q25 - (iqr*1.5)\n",
    "    max_val = q75 + (iqr*1.5)\n",
    "    \n",
    "    # count number of outliers, which are the data that are less than min_val or more than max_val calculated above\n",
    "    outlier_count = len(np.where((data[col] > max_val) | (data[col] < min_val))[0])\n",
    "    \n",
    "    # calculate the percentage of the outliers\n",
    "    outlier_percent = round(outlier_count/len(data[col])*100, 2)\n",
    "    \n",
    "    if(outlier_count > 0):\n",
    "        print(\"\\n\"+15*'-' + col + 15*'-'+\"\\n\")\n",
    "        print('Number of outliers: {}'.format(outlier_count))\n",
    "        print('Percent of data that is outlier: {}%'.format(outlier_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daac8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train.columns:\n",
    "    outlier_count(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3f8db3b",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39937bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# feature scaling helps improve reach convergence faster\n",
    "scaler = StandardScaler() # for standadization use StandardScaler() , for normalization use MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "#x = (x - mean) / std\n",
    "#why do we want to scale our data before data analysis / machine learning\n",
    "\n",
    "#allows your machine learning model to catch the pattern/relationship faster\n",
    "#faster convergence\n",
    "\n",
    "#how many ways to scale\n",
    "#standardardization <====current way\n",
    "# (x - mean) / std\n",
    "#--> when your data follows normal distribution\n",
    "\n",
    "#normalization <---another way\n",
    "# (x - x_min) / (x_max - x_min)\n",
    "#---> when your data DOES NOT follow normal distribution (e.g., audio, signal, image) We will use nomalization when mean is a bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check shapes of all X_train, X_test, y_train, y_test\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37ec9dd2",
   "metadata": {},
   "source": [
    "## 6. Modeling\n",
    "\n",
    "Let's define some algorithms and compare them using cross-validation.\n",
    "\n",
    "[Scikit-Learn](http://scikit-learn.org) provides quick access to a huge pool of machine learning algorithms.\n",
    "\n",
    "Before using sklearn, there is **one thing you need to know**, i.e., the **data shape that sklearn wants**.\n",
    "\n",
    "To apply majority of the algorithms, sklearn requires two inputs, i.e., $\\mathbf{X}$ and $\\mathbf{y}$.\n",
    "\n",
    "-  $\\mathbf{X}$, or the **feature matrix** *typically* has the shape of ``[n_samples, n_features]``\n",
    "-  $\\mathbf{y}$, or the **target/label vector** *typically* has the shape of ``[n_samples, ]`` or ``[n_samples, n_targets]`` depending whether that algorithm supports multiple labels\n",
    "\n",
    "Note 1:  if you $\\mathbf{X}$ has only 1 feature, the shape must be ``[n_samples, 1]`` NOT ``[n_samples, ]``\n",
    "\n",
    "Note 2:  sklearn supports both numpy and pandas, as long as the shape is right.  For example, if you use pandas, $\\mathbf{X}$ would be a dataframe, and $\\mathbf{y}$ could be a series or dataframe.\n",
    "\n",
    "Tips:  it's always better to look at sklearn documentation before applying any algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0efd13a",
   "metadata": {},
   "source": [
    "### Much better: Cross validation + Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070e6e1",
   "metadata": {},
   "source": [
    "To find the appropriate algorithms, We need to choose what algorithms good for prediction. This step calls Cross validation.\n",
    "Algorithms have named for using such as \n",
    "\n",
    "1.Linear Regression      \n",
    "2.SVR      \n",
    "3.KNeighbors Regressor      \n",
    "4.Decision-Tree Regressor      \n",
    "5.Random-Forest Regressor     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  #we are using regression models\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Libraries for model evaluation\n",
    "\n",
    "# models that we will be using, put them in a list\n",
    "algorithms = [LinearRegression(), SVR(), KNeighborsRegressor(), DecisionTreeRegressor(random_state = 0), \n",
    "              RandomForestRegressor(n_estimators = 100, random_state = 0)]\n",
    "\n",
    "# The names of the models\n",
    "algorithm_names = [\"Linear Regression\", \"SVR\", \"KNeighbors Regressor\", \"Decision-Tree Regressor\", \"Random-Forest Regressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a8e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  #we are using regression models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "print(\"MSE: \", mean_squared_error(y_test, yhat))\n",
    "print(\"r2: \", r2_score(y_test, yhat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "473e2531",
   "metadata": {},
   "source": [
    "Let's do some simple cross-validation here...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff4b1c6",
   "metadata": {},
   "source": [
    "The next step is spilt the training set to 5 fold for one iteration. There will be Test set(validation set) and Training set. In each iteration, we will get mse for one value and next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85366d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "#lists for keeping mse\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "#defining splits\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for i, model in enumerate(algorithms):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error') #Higher is better.\n",
    "    print(f\"{algorithm_names[i]} - Score: {scores}; Mean: {scores.mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeb53bde",
   "metadata": {},
   "source": [
    "Hmm...it seems random forest do very well....how about we grid search further to find the best version of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6db1e5e9",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a220979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'bootstrap': [True], 'max_depth': [5, 10, None],\n",
    "              'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 1)\n",
    "\n",
    "grid = GridSearchCV(estimator = rf, \n",
    "                    param_grid = param_grid, \n",
    "                    cv = kfold, \n",
    "                    n_jobs = -1, \n",
    "                    return_train_score=True, \n",
    "                    refit=True,\n",
    "                    scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit your grid_search\n",
    "grid.fit(X_train, y_train);  #fit means start looping all the possible parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae371c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find your grid_search's best score\n",
    "best_mse = grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse  # ignore the minus because it's neg_mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ff42019",
   "metadata": {},
   "source": [
    "## 7. Testing\n",
    "\n",
    "Of course, once we do everything.  We can try to shoot with the final test set.  We should no longer do anything like improving the model.  It's illegal!  since X_test is the final final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f82a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = grid.predict(X_test)\n",
    "mean_squared_error(y_test,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72562afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original = np.exp(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b70535",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_original)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6074023",
   "metadata": {},
   "source": [
    "## 8. Analysis:  Feature Importance\n",
    "\n",
    "Understanding why is **key** to every business, not how low MSE we got.  Extracting which feature is important for prediction can help us interpret the results.  There are several ways: algorithm, permutation, and shap.  Note that these techniques can be mostly applied to most algorithms. \n",
    "\n",
    "Most of the time, we just apply all, and check the consistency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "073fb9a1",
   "metadata": {},
   "source": [
    "#### Algorithm way\n",
    "\n",
    "Some ML algorithms provide feature importance score after you fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bed781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stored in this variable\n",
    "#note that grid here is random forest\n",
    "rf = grid.best_estimator_\n",
    "\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828da6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot\n",
    "plt.barh(X.columns, rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88aac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmm...let's sort first\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "plt.barh(X.columns[sorted_idx], rf.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Random-Forest Regressor Importance\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2398c6b",
   "metadata": {},
   "source": [
    "#### Permutation way\n",
    "\n",
    "This method will randomly shuffle each feature and compute the change in the model’s performance. The features which impact the performance the most are the most important one.\n",
    "\n",
    "*Note*: The permutation based importance is computationally expensive. The permutation based method can have problem with highly-correlated features, it can report them as unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(rf, X_test, y_test)\n",
    "\n",
    "#let's plot\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(X.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Random-Forest Regressor Importance\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d076c453",
   "metadata": {},
   "source": [
    "#### Shap way\n",
    "\n",
    "The SHAP interpretation can be used (it is model-agnostic) to compute the feature importances. It is using the Shapley values from game theory to estimate the how does each feature contribute to the prediction. It can be easily installed (<code>pip install shap</code>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap provides plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names = X.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e5ac18e",
   "metadata": {},
   "source": [
    "## 9. Inference\n",
    "\n",
    "To provide inference service or deploy, it's best to save the model for latter use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'model/price_car_prediction1.model'\n",
    "pickle.dump(grid, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543edf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcfc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try to create one silly example\n",
    "df[['max_power','mileage','seats', 'km_driven', 'owner', 'lable_fuel','selling_price']].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array([[74,23.4,5,145500,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a49f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "selling_price = loaded_model.predict(sample)\n",
    "selling_price = np.exp(selling_price)\n",
    "print(selling_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908bed53",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd1905",
   "metadata": {},
   "source": [
    "_This model is about car price prediction.It is the regression problem because the label, selling price, is continuous. There are a lots of datas in .CSV file. In the CSV file, there are name year selling_price km_driven fuel seller_type transmission mileage engine max_power torque seats.To deal with this datas we need to manage and understand the nature of the datas. For predicting prices of cars we not only trusted the correlation matrix or PPS but also understand the reality of features that have affect to selling prices. If we already understand the reality of the data we can choose the datas that have related to selling prices. In these notebook will choose 6 important features that have affected such as max_power, mileage, seats, km_driven, owner, fuel. Why we choose all this features? because when plotting the graph as we can see that we will discover what features are good or not good. The good will have related with the label,selling prices and the bad one is not related to selling prices not only for plotting but also looking to correlation matrix so that all the feature that I have choosen. when we already choosen the features, We need to decide the label that is selling prices. The next step, We will check the missing values of the data, To filling the missing value, We will fill with mean or median because we don't want the high MSE(Mean Squere Error). After that we will check the outliers and scales the datas. The scaling have 2 methods that is standadization and normalization. We will use standadization for scaling data when the data follow normal distribution and for the normalization when the data don't follow normalization so this is the method calls preprocessing. The next steps is Modeling. In this steps we will find the precise and accurate algorithm. There are the algoritms names : \"Linear Regression\", \"SVR\", \"KNeighbors Regressor\", \"Decision-Tree Regressor\", \"Random-Forest Regressor\" we will explain each of the algorithm next paragraph so the step that find algoritms calls Cross-validation and to do the best version of algorithm we calls Grid search after that we will testing to predict selling prices from the data by training and testing set from previous section._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1307031a",
   "metadata": {},
   "source": [
    "In this case, The algorithm that perform well is Random-Forest Regression. because the mean of MSE is the highest from all algoritms. For picking the best algorithm, We will look to the mean of MSE of each algoritm. If mean of MSE in each algorithm is higher, the better to choose algorithm is that so we will explain what is each of algoritms that we have.\n",
    "1. **Linear Regression** -> is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.       \n",
    "2. **SVR** ->  is a machine learning algorithm that extends the principles of Support Vector Machines (SVMs) to the task of regression. While SVMs are primarily used for classification tasks, SVR is designed for predicting continuous numeric values. SVR aims to find a regression function that fits the data while controlling the margin of error.\n",
    "3. **KNeighbors Regressor** ->  is a supervised machine learning algorithm used for regression tasks. It is a non-parametric and instance-based algorithm that makes predictions based on the similarity of input data points to their neighboring data points. KNeighborsRegressor is an example of a lazy learner, meaning it doesn't build an explicit model during training, but instead, it stores the training data and uses it directly for making predictions.     \n",
    "4. **Decision-Tree Regressor** -> is a supervised machine learning algorithm used for regression tasks. It's a type of decision tree algorithm that predicts a continuous target variable based on the features of the input data. Decision trees are versatile and easy to understand, making them a popular choice for both classification and regression problems.      \n",
    "5. **Random-Forest Regressor** ->  is a machine learning algorithm that belongs to the ensemble learning family. It's designed for regression tasks and is an extension of the concept of decision trees. The key idea behind a Random Forest Regressor is to build multiple decision trees and combine their predictions to improve overall accuracy and reduce overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
